# ğŸ§  CUDA Memory Hierarchy - Overview & Quick Reference

Understanding the CUDA memory hierarchy is **critical** for writing performant kernels. The choice of memory and access pattern dramatically impacts latency, bandwidth utilization, and overall throughput.

## ğŸ“š Navigation Guide

### ğŸ”— **Detailed Section Files**
- **[ğŸ“‹ Memory Types Deep Dive](2a_memory_types_detailed.md)** - Complete explanations of all memory types
- **[ğŸŒ Global Memory Optimization](2b_global_memory_advanced.md)** - Coalescing patterns and performance analysis  
- **[âš¡ Shared Memory Complete Guide](2c_shared_memory_complete.md)** - Matrix multiplication, bank conflicts, benchmarks
- **[ğŸ“¥ Constant Memory Complete Guide](2d_constant_memory_complete.md)** - Domain examples, optimization strategies
- **[ğŸ” Unified Memory Complete Guide](2e_unified_memory_complete.md)** - Advanced techniques, multi-GPU, performance analysis
- **[ğŸ›  Memory Debugging Toolkit](2f_memory_debugging_complete.md)** - Troubleshooting workflows and profiling strategies

---

## ğŸ§  **Memory Types Complete Reference**

| Memory Type | Latency | Size Limit | Scope | Lifetime | Access Pattern | Best For | Detailed Guide |
|-------------|---------|------------|-------|----------|----------------|----------|----------------|
| **Registers** | ~1 cycle | 32-255/thread | Thread | Kernel | Private to thread | Scalars, loop counters, temporary values | [ğŸ“‹ Types Guide](2a_memory_types_detailed.md#registers) |
| **Local Memory** | 300-600 cycles | Unlimited | Thread | Kernel | Private (in global mem) | Arrays/structs too large for registers | [ğŸ“‹ Types Guide](2a_memory_types_detailed.md#local-memory) |
| **Shared Memory** | ~2-3 cycles | 48-164 KB/block | Block | Kernel | Shared within block | Tile storage, cooperative block computation | [âš¡ Shared Guide](2c_shared_memory_complete.md) |
| **Global Memory** | 300-600 cycles | GPU VRAM | All Threads | Application | Read/Write by all | Main memory for large input/output datasets | [ğŸŒ Global Guide](2b_global_memory_advanced.md) |
| **Constant Memory** | ~1 cycle (cached) | 64 KB total | All Threads | Application | Broadcast to all (RO) | Lookup tables, config params, small readonly data | [ğŸ“¥ Constant Guide](2d_constant_memory_advanced.md) |
| **Texture/Surface** | ~1-10 cycles (cached) | GPU VRAM | All Threads | Application | Specialized read/write | 2D/3D spatial data, image processing | [ğŸ“‹ Types Guide](2a_memory_types_detailed.md#texture-memory) |
| **Unified Memory** | Variable | System RAM+GPU | CPU + GPU | Application | Auto-migrated host/device | Rapid development, shared CPU-GPU access | [ğŸ” Unified Guide](2e_unified_memory_advanced.md) |


#### ğŸ”¸ Registers
Registers provide the most efficient, low-latency storage available to CUDA threads. They are allocated per thread and are ideal for **frequently reused** values such as loop counters, temporary calculations, and scalar variables. However, registers are limited in number per thread, and excess usage causes register spills into **local memory**, which significantly degrades performance.

#### ğŸ”¸ Local Memory
Despite its name, local memory actually resides in **global memory** and has similar high latency characteristics. This memory type is used for large thread-local arrays and register spills when the number of registers exceeds the hardware limit. Accesses to local memory are slower and uncached unless explicitly optimized, making it important to minimize its usage.

#### ğŸ”¸ Shared Memory
Shared memory resides **on-chip** and provides very low latency access. It is visible and writable to all threads within a block, making it perfect for tiling algorithms (such as matrix multiplication) and inter-thread communication within a block. To achieve optimal performance, developers must carefully manage **bank conflicts** that can serialize memory accesses.

#### ğŸ”¸ Global Memory
Global memory serves as the primary GPU memory (DRAM) and has high latency of approximately 300â€“600 cycles. It is accessible by all threads and can be accessed by the CPU through explicit memory copies or unified memory. The key optimization for global memory is ensuring **coalesced access** patterns to reduce transaction overhead and maximize memory bandwidth utilization.

#### ğŸ”¸ Constant Memory
Constant memory is read-only memory that is cached and broadcast-efficient across all threads. It is size-limited (typically 64 KB) and is ideal for storing uniform values such as configuration parameters, coefficients, and lookup tables. This memory type is accessed via the `__constant__` qualifier and provides excellent performance when the same data is read by multiple threads.

#### ğŸ”¸ Texture/Surface Memory
Texture and surface memory are specialized for 2D/3D spatial access patterns and are commonly used in image and signal processing applications. They offer built-in **caching**, **addressing**, and **interpolation** features that can significantly improve performance for spatially coherent data access patterns.

#### ğŸ”¸ Unified Memory
Unified memory abstracts away explicit `cudaMemcpy` operations by providing system-managed page migration between CPU and GPU. While this makes programming easier, performance can suffer from page faults and unpredictable behavior if memory is not properly prefetched using techniques like `cudaMemPrefetchAsync()`.


### ğŸ” Why It Matters

The choice of memory directly influences performance and developer productivity.

| Goal                           | Memory Type Recommendation                          |
|--------------------------------|-----------------------------------------------------|
| Maximum speed per thread       | Registers                                            |
| Intra-block communication      | Shared Memory                                        |
| Global data sharing            | Global or Unified Memory                             |
| Read-only broadcast values     | Constant Memory                                      |
| Host-device unified access     | Unified Memory                                       |
| 2D/3D data with locality       | Texture / Surface Memory                             |

Understanding these trade-offs enables you to:
- Avoid unnecessary latency.
- Exploit fast memory tiers (registers/shared).
- Design scalable, high-performance kernels.


### ğŸ§  Director-Level Insight

> â€œWe re-architected a batched inference kernel to shift from global to shared memory for intermediate tensors, resulting in a 4.2x speedup on A100. We also used constant memory for weights shared across warps to improve L1 utilization.â€


## âš¡ **Performance Quick Reference**

### ğŸ¯ **Memory Bandwidth Efficiency**
```
Theoretical Peak: ~1000 GB/s (RTX 4090)

Achieved Performance by Optimization:
â”œâ”€â”€ Naive Global Access:     124 GB/s (12% efficiency)
â”œâ”€â”€ Coalesced Access:        201 GB/s (20% efficiency)  
â”œâ”€â”€ Shared Memory Tiled:     487 GB/s (48% efficiency)
â””â”€â”€ Optimized + No Conflicts: 603 GB/s (60% efficiency)
```

### ğŸš€ **Common Optimization Impact**
| Optimization | Typical Speedup | When to Use |
|--------------|----------------|-------------|
| **Coalesced Access** | 1.5-3x | Always for global memory |
| **Shared Memory Tiling** | 3-8x | Reused data within blocks |
| **Bank Conflict Fix** | 1.2-2x | Column-wise shared access |
| **Constant Memory** | 2-50x | Small uniform read-only data |
| **Unified Memory + Prefetch** | 1.5-4x | CPU-GPU workflows |

## ğŸ”§ **Essential Code Patterns**

### âœ… **Coalesced Global Memory Access**
```cpp
// Good: Consecutive threads â†’ consecutive addresses
int idx = threadIdx.x + blockIdx.x * blockDim.x;
float val = data[idx];  // Perfect coalescing
```

### âš¡ **Shared Memory Template**
```cpp
__global__ void optimized_kernel(float* input, float* output) {
    __shared__ float tile[TILE_SIZE][TILE_SIZE + 1]; // +1 prevents bank conflicts
    
    // Load phase
    tile[threadIdx.y][threadIdx.x] = input[...]; // Coalesced load
    __syncthreads();
    
    // Compute phase  
    float result = 0;
    for (int i = 0; i < TILE_SIZE; i++) {
        result += tile[threadIdx.y][i] * tile[i][threadIdx.x]; // Fast shared access
    }
    __syncthreads();
    
    // Store phase
    output[...] = result; // Coalesced store
}
```

### ğŸ“¥ **Constant Memory Pattern**
```cpp
__constant__ float lookup_table[1024];

// Host: Initialize once
cudaMemcpyToSymbol(lookup_table, host_data, sizeof(host_data));

// Device: Fast broadcast access
__device__ float fast_lookup(int index) {
    return lookup_table[index]; // All threads get same value efficiently
}
```

### ğŸ” **Unified Memory Pattern**
```cpp
// Allocation
float* data;
cudaMallocManaged(&data, size);

// Optimization hints
cudaMemAdvise(data, size, cudaMemAdviseSetPreferredLocation, 0); // GPU
cudaMemPrefetchAsync(data, size, 0); // Move to GPU before kernel

// Use from both CPU and GPU without explicit copies
kernel<<<blocks, threads>>>(data);
process_results_cpu(data);
```

## ğŸš¨ **Common Performance Issues & Quick Fixes**

### âŒ **Issue: Poor Global Memory Performance**
**Symptom:** `gld_efficiency < 50%` in profiler
**Quick Fix:** Ensure consecutive threads access consecutive memory
```cpp
// Bad: Strided access
data[threadIdx.x * stride]

// Good: Sequential access  
data[threadIdx.x + blockIdx.x * blockDim.x]
```

### âŒ **Issue: Shared Memory Bank Conflicts**
**Symptom:** `shared_load_transactions_per_request > 1.0`
**Quick Fix:** Add padding to shared arrays
```cpp
// Bad: 32-way conflicts on column access
__shared__ float tile[32][32];

// Good: Conflict-free with padding
__shared__ float tile[32][33]; // +1 element padding
```

### âŒ **Issue: Unified Memory Page Faults**
**Symptom:** Irregular first-run performance
**Quick Fix:** Prefetch before kernel launch
```cpp
cudaMemPrefetchAsync(data, size, 0); // Move to GPU
kernel<<<blocks, threads>>>(data);
```

## ğŸ“Š **Profiling Quick Commands**

### ğŸ” **Memory Coalescing Check**
```bash
ncu --metrics gld_efficiency,gst_efficiency ./app
# Target: > 80%
```

### ğŸ” **Shared Memory Conflicts**
```bash
ncu --metrics shared_load_transactions_per_request ./app  
# Target: < 1.1
```

### ğŸ” **Overall Memory Performance**
```bash
ncu --metrics dram__throughput.avg.pct_of_peak_sustained_elapsed ./app
# Target: > 60% for memory-bound kernels
```

### ğŸ” **Unified Memory Analysis**
```bash
nsys profile --trace=cuda ./app
# Look for: Page fault patterns in timeline view
```

## ğŸ¯ **Optimization Decision Tree**

```
Memory Performance Issue?
â”œâ”€â”€ Low global memory efficiency?
â”‚   â”œâ”€â”€ Fix coalescing â†’ [ğŸŒ Global Guide](2b_global_memory_advanced.md)
â”‚   â””â”€â”€ Use shared memory tiling â†’ [âš¡ Shared Guide](2c_shared_memory_complete.md)
â”œâ”€â”€ Shared memory bank conflicts?
â”‚   â””â”€â”€ Add padding, fix access patterns â†’ [âš¡ Shared Guide](2c_shared_memory_complete.md#bank-conflicts)
â”œâ”€â”€ Small uniform read-only data?
â”‚   â””â”€â”€ Use constant memory â†’ [ğŸ“¥ Constant Guide](2d_constant_memory_advanced.md)  
â”œâ”€â”€ Complex CPU-GPU workflows?
â”‚   â””â”€â”€ Optimize unified memory â†’ [ğŸ” Unified Guide](2e_unified_memory_advanced.md)
â””â”€â”€ Still having issues?
    â””â”€â”€ Systematic debugging â†’ [ğŸ›  Debug Guide](2f_memory_debugging_complete.md)
```

## ğŸ§  **Key Principles**

1. **ğŸ¯ Coalesce First**: Always ensure global memory accesses are coalesced
2. **âš¡ Reuse in Shared**: Move frequently reused data to shared memory  
3. **ğŸ“¥ Broadcast Constants**: Use constant memory for uniform read-only data
4. **ğŸ”§ Profile Everything**: Use Nsight Compute/Systems to validate optimizations
5. **ğŸ“š Understand Trade-offs**: Sometimes lower occupancy = better performance

## ğŸ’¡ **Next Steps**

1. **Start with**: [ğŸŒ Global Memory Guide](2b_global_memory_advanced.md) for coalescing basics
2. **Then learn**: [âš¡ Shared Memory Guide](2c_shared_memory_complete.md) for advanced tiling
3. **Specialize with**: [ğŸ“¥ Constant](2d_constant_memory_advanced.md) or [ğŸ” Unified](2e_unified_memory_advanced.md) memory guides
4. **Debug with**: [ğŸ›  Debugging Toolkit](2f_memory_debugging_complete.md) when performance issues arise

---

**ğŸ’¡ Pro Tip**: Start simple with coalesced global memory, then progressively add shared memory optimizations. Profile at each step to quantify improvements!


