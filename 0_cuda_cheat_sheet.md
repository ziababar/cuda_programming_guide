# CUDA Technical Mastery ‚Äì Cheat Sheet


## 1. üèóÔ∏è GPU Architecture

| Concept | Description | Key Notes |
|---------|-------------|-----------|
| **GPU** | Graphics Processing Unit with many parallel cores | Contains multiple SMs (e.g., A100 has 108 SMs) |
| **SM** | Streaming Multiprocessor - execution unit | Contains CUDA cores, shared mem, registers |
| **CUDA Core** | Basic arithmetic unit within SM | Executes individual thread instructions |
| **Warp Scheduler** | Hardware that manages warp execution | 4 warp schedulers per SM (typical) |
| **Memory Controllers** | Manage access to global memory | Determine memory bandwidth |
| **Tensor Cores** | Specialized units for AI workloads | Mixed-precision matrix operations |

üìå **Best Practice**: Choose GPU based on workload requirements (compute vs memory intensive).

### üèóÔ∏è **Hardware Hierarchy**
```
         GPU Device
              |
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   SM-0     SM-1    ... SM-N
    |         |           |
‚îå‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îê
C-0...C-N C-0...C-N   C-0...C-N
(CUDA Cores per SM)
```

**Key Specifications (Example: A100)**:
- **SMs**: 108 Streaming Multiprocessors
- **CUDA Cores**: 64 per SM (6,912 total)
- **Tensor Cores**: 4 per SM (432 total)
- **Memory**: 40-80GB HBM2e
- **Bandwidth**: 1.6TB/s memory bandwidth

**Architecture Impact**:
- **More SMs** = More parallel blocks
- **More CUDA cores per SM** = More threads per block
- **Higher memory bandwidth** = Better data throughput
- **Tensor cores** = Accelerated AI/ML workloads




## 2. üö¶ Execution Model

| Concept           | Description                                           | Key Notes                                             |
|------------------|-------------------------------------------------------|--------------------------------------------------------|
| **Thread**        | Basic unit of execution                              | Executes kernel code on CUDA cores                   |
| **Warp**          | Group of 32 threads executed together                | Warp divergence hurts performance                     |
| **Block**         | Group of warps assigned to an SM                     | Shares **shared memory**, can sync via `__syncthreads()` |
| **Grid**          | Collection of blocks distributed across SMs          | Can be 1D, 2D, 3D                                     |
| **Indexing**      | `threadIdx`, `blockIdx`, `blockDim`, `gridDim`       | Used to access per-thread data                        |

üìå **Best Practice**: Align your kernel design to problem geometry (e.g. 2D grid for images).

![Grid of Thread Blocks](../images/grid-of-thread-blocks.png)

**Execution Flow:**
1. **Kernel Launch**: Host launches a grid of thread blocks
2. **Block Distribution**: CUDA runtime distributes blocks across available SMs
3. **Warp Scheduling**: Each SM schedules warps (groups of 32 threads) for execution
4. **Thread Execution**: Individual threads execute kernel code in parallel

**Key Relationships:**
- Multiple **blocks** can run on a single **SM** (if resources allow)
- Each **block** is divided into **warps** of 32 consecutive threads
- All threads in a **warp** execute the same instruction (SIMT model)
- **Blocks** within a **grid** can execute in any order (independence requirement)

### üîß **Kernel Fundamentals**
| Concept | Description | Key Notes |
|---------|-------------|-----------|
| **Kernel** | `__global__` function executed by all threads in parallel | Uses `<<<grid, block>>>` launch syntax |
| **Launch Config** | `kernel<<<blocks, threads>>>(args)` | Defines grid and block dimensions |
| **Thread Index** | Each thread calculates unique ID for data access | `int id = blockIdx.x * blockDim.x + threadIdx.x` |

**Example**:
```cpp
__global__ void vectorAdd(float* A, float* B, float* C, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) C[i] = A[i] + B[i];
}
// Launch: vectorAdd<<<blocks, threads>>>(d_A, d_B, d_C, N);
```



## 3. üß† Memory Hierarchy

| Type            | Scope     | Speed     | Use Case                                     |
|-----------------|-----------|-----------|----------------------------------------------|
| **Registers**    | Thread    | Fastest   | Scalar/local variables                        |
| **Shared Mem**   | Block     | Very Fast | Thread collaboration within block             |
| **Global Mem**   | Grid      | Slow      | Main GPU memory                               |
| **Constant Mem** | Grid      | Fast (RO) | Broadcast constants to all threads            |
| **Texture/Surface** | Grid   | Optimized | 2D spatial locality, read-only                |
| **Unified Mem**  | Grid/Host | Medium    | Simplified allocation, slower for frequent access |

üìå **Best Practice**:
- Coalesce global memory access
- Use shared memory to reduce global loads
- Avoid shared memory **bank conflicts**


![Memory Hierarchy](../images/memory-hierarchy.png)

**Memory Access Patterns:**
1. **Registers**: Fastest access, private to each thread, limited capacity (~64KB per SM)
2. **Shared Memory**: Fast inter-thread communication within a block, ~48-164KB per SM
3. **L1/L2 Cache**: Hardware-managed caching of global memory accesses
4. **Global Memory**: Main GPU DRAM, highest capacity but slowest access
5. **Constant Memory**: Read-only, cached, excellent for broadcasting small data
6. **Texture Memory**: Optimized for 2D spatial locality with built-in filtering

**Performance Guidelines:**
- **Maximize register usage** for frequently accessed variables
- **Use shared memory** for data sharing between threads in a block
- **Ensure coalesced access** to global memory (consecutive threads access consecutive addresses)
- **Minimize global memory traffic** by reusing data through shared memory or registers



## 4. ‚öôÔ∏è Streams & Concurrency

| Concept              | Description                                              |
|----------------------|----------------------------------------------------------|
| **Streams**           | Command queues for async operations                      |
| **Default stream**    | Implicitly synchronizes                                  |
| **Multiple streams**  | Enable overlapping memcpy & compute                      |
| **Stream priorities** | Control execution ordering                               |
| **CUDA Graphs**       | Capture & replay workflows with less overhead            |

üìå **Best Practice**:
- Use pinned host memory (`cudaHostAlloc`) for async memcpy
- Use `cudaMemcpyAsync()` and multiple streams for throughput

---

## 5. üìà Profiling & Optimization

| Tool               | Use Case                             |
|--------------------|--------------------------------------|
| `nvprof`           | Basic kernel and memory profiling    |
| **Nsight Compute** | Kernel-level performance analysis    |
| **Nsight Systems** | Full system profiling (timeline)     |
| `cuda-gdb`         | Debugger for CUDA code               |

### üîç Optimization Metrics
- **Occupancy**: Threads/SM efficiency
- **Memory Throughput**: Avoid serialization
- **Warp Efficiency**: Avoid divergence
- **Execution Dependencies**: Hide latency with async

üìå **Best Practice**:
- Balance compute vs memory (Roofline model)
- Use shared memory tiling for matrix ops

---

## 6. üß¨ Advanced Features

| Feature              | Description                                      |
|----------------------|--------------------------------------------------|
| **Cooperative Groups**| Fine-grained synchronization within warps/groups|
| **Dynamic Parallelism**| Launch kernels from within kernels              |
| **Tensor Cores**      | Fused matrix-multiply for DL (use `mma.sync`)   |
| **cuBLAS/cuDNN/cuFFT**| Optimized CUDA libraries                        |

üìå **Best Practice**:
- Use libraries when possible over hand-coded kernels
- Use `cudaFuncAttributes` to tune resource use

---

## 7. üåê Multi-GPU & Unified Memory

| Concept               | Details                                            |
|------------------------|----------------------------------------------------|
| **P2P Access**         | Direct access between GPUs via NVLink              |
| **cudaMemcpyPeer**     | Fast inter-GPU data transfer                       |
| **Unified Memory**     | `cudaMallocManaged` for simplified pointer mgmt    |
| **Prefetching**        | Use `cudaMemPrefetchAsync()` for hinting data loc  |

üìå **Best Practice**:
- Avoid relying solely on Unified Memory for perf-sensitive workloads
- Enable ECC for mission-critical enterprise workloads

---

## ‚úÖ Key Code Snippets

```cpp
// Thread Indexing (2D Grid)
int x = blockIdx.x * blockDim.x + threadIdx.x;
int y = blockIdx.y * blockDim.y + threadIdx.y;

// Shared Memory Usage
__shared__ float tile[32][32];
tile[threadIdx.y][threadIdx.x] = input[y][x];
__syncthreads();

// Async Copy
cudaMemcpyAsync(dst, src, size, cudaMemcpyHostToDevice, stream);
```

---

## üéØ Director-Level Talking Points

| Topic                          | Insight Example                                                                 |
|-------------------------------|----------------------------------------------------------------------------------|
| Strategic CUDA adoption       | ‚ÄúWe shifted a genomics pipeline to CUDA, reducing processing time by 80%.‚Äù     |
| Vendor/platform optimization  | ‚ÄúBenchmarked A100 vs L40S to guide GPU fleet investments for mixed workloads.‚Äù |
| GPU-aware system design       | ‚ÄúUsed graph capture + multi-stream execution for NLP inference batching.‚Äù       |
| Team enablement               | ‚ÄúMentored team on using Nsight tools to identify warp inefficiencies.‚Äù         |